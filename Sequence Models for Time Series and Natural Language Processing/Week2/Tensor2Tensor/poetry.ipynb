{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation using tensor2tensor on Cloud ML Engine\n",
    "\n",
    "This notebook illustrates using the <a href=\"https://github.com/tensorflow/tensor2tensor\">tensor2tensor</a> library to do from-scratch, distributed training of a poetry model. Then, the trained model is used to complete new poems.\n",
    "\n",
    "\n",
    "<p/>\n",
    "### Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary packages. tensor2tensor will give us the Transformer model. Project Gutenberg gives us access to historical poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard==1.8.0\n",
      "tensorflow==1.8.0\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor2tensor\n",
      "  Using cached https://files.pythonhosted.org/packages/6d/ac/6897d0cb0bc1701cf5b5a56afc5c24a587c34b46e73c1f732c7d2cb9f54d/tensor2tensor-1.10.0-py2.py3-none-any.whl\n",
      "Collecting gutenberg\n",
      "  Using cached https://files.pythonhosted.org/packages/14/b1/6e99867c38e70d46366966a0a861c580377f38312cf9dbad38b82ed1823d/Gutenberg-0.7.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: oauth2client in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (2.2.0)\n",
      "Collecting tqdm (from tensor2tensor)\n",
      "  Using cached https://files.pythonhosted.org/packages/91/55/8cb23a97301b177e9c8e3226dba45bb454411de2cbd25746763267f226c2/tqdm-4.28.1-py2.py3-none-any.whl\n",
      "Collecting gym (from tensor2tensor)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (1.10.0)\n",
      "Collecting gevent (from tensor2tensor)\n",
      "  Downloading https://files.pythonhosted.org/packages/40/4f/222cefc08c1ffda69454908496e46c32f7b82da30ade4861178c6a72405e/gevent-1.3.7-cp27-cp27mu-manylinux1_x86_64.whl (4.2MB)\n",
      "Requirement already satisfied, skipping upgrade: flask in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (0.11.1)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (0.16.0)\n",
      "Collecting mesh-tensorflow (from tensor2tensor)\n",
      "  Using cached https://files.pythonhosted.org/packages/fd/f0/f30350bf2a7e6b92af93e1b0dd51def3ef38da69feb9363e06c3b87082ce/mesh_tensorflow-0.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-python-client in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (1.6.2)\n",
      "Requirement already satisfied, skipping upgrade: sympy in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (0.7.6.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (2.18.4)\n",
      "Collecting bz2file (from tensor2tensor)\n",
      "  Using cached https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: gunicorn in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (19.9.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensor2tensor) (2.7.1)\n",
      "Collecting rdflib-sqlalchemy>=0.3.8 (from gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/76/f4613574cc557262861dca47e2c5fc8f2435f75424acaa73f8517aed1346/rdflib-sqlalchemy-0.3.8.tar.gz (1.2MB)\n",
      "Collecting rdflib>=4.2.0 (from gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/77/1fa0f4cffd5faad496b1344ab665902bb2609f56e0fb19bcf80cff485da0/rdflib-4.2.2.tar.gz (905kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from gutenberg) (40.0.0)\n",
      "Requirement already satisfied, skipping upgrade: httplib2>=0.9.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client->tensor2tensor) (0.11.3)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client->tensor2tensor) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client->tensor2tensor) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client->tensor2tensor) (3.4.2)\n",
      "Collecting pyglet>=1.2.0 (from gym->tensor2tensor)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\" (from gevent->tensor2tensor)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/6e/f2d25875713ad0885c8d3c69269697406652e6f64e1a6bd8264f7a609327/greenlet-0.4.15-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.21 in /usr/local/envs/py2env/lib/python2.7/site-packages (from flask->tensor2tensor) (0.24)\n",
      "Requirement already satisfied, skipping upgrade: click>=2.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from flask->tensor2tensor) (6.7)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from flask->tensor2tensor) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from flask->tensor2tensor) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-api-python-client->tensor2tensor) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests->tensor2tensor) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests->tensor2tensor) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests->tensor2tensor) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests->tensor2tensor) (2018.8.13)\n",
      "Requirement already satisfied, skipping upgrade: alembic>=0.8.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (0.8.10)\n",
      "Requirement already satisfied, skipping upgrade: SQLAlchemy>=1.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from rdflib-sqlalchemy>=0.3.8->gutenberg) (1.2.10)\n",
      "Collecting isodate (from rdflib>=4.2.0->gutenberg)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing in /usr/local/envs/py2env/lib/python2.7/site-packages (from rdflib>=4.2.0->gutenberg) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe in /usr/local/envs/py2env/lib/python2.7/site-packages (from Jinja2>=2.4->flask->tensor2tensor) (1.0)\n",
      "Requirement already satisfied, skipping upgrade: Mako in /usr/local/envs/py2env/lib/python2.7/site-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: python-editor>=0.3 in /usr/local/envs/py2env/lib/python2.7/site-packages (from alembic>=0.8.8->rdflib-sqlalchemy>=0.3.8->gutenberg) (1.0.3)\n",
      "Building wheels for collected packages: gutenberg, gym, bz2file, rdflib-sqlalchemy, rdflib\n",
      "  Running setup.py bdist_wheel for gutenberg: started\n",
      "  Running setup.py bdist_wheel for gutenberg: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/8e/cd/75/4bc6f16541a1b7a69b02168da567695b2271c23ac4a0a0a453\n",
      "  Running setup.py bdist_wheel for gym: started\n",
      "  Running setup.py bdist_wheel for gym: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
      "  Running setup.py bdist_wheel for bz2file: started\n",
      "  Running setup.py bdist_wheel for bz2file: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "  Running setup.py bdist_wheel for rdflib-sqlalchemy: started\n",
      "  Running setup.py bdist_wheel for rdflib-sqlalchemy: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/8c/7f/e5/2cfedb41ce9149b8af8c35a063b4fd1b8c1573c6391bbd5b53\n",
      "  Running setup.py bdist_wheel for rdflib: started\n",
      "  Running setup.py bdist_wheel for rdflib: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/8d/f6/b7/f5e9501d0f006fc9fd497c930206952856b2191ab5c836cb97\n",
      "Successfully built gutenberg gym bz2file rdflib-sqlalchemy rdflib\n",
      "Installing collected packages: tqdm, pyglet, gym, greenlet, gevent, mesh-tensorflow, bz2file, tensor2tensor, isodate, rdflib, rdflib-sqlalchemy, gutenberg\n",
      "Successfully installed bz2file-0.98 gevent-1.3.7 greenlet-0.4.15 gutenberg-0.7.0 gym-0.10.9 isodate-0.6.0 mesh-tensorflow-0.0.3 pyglet-1.3.2 rdflib-4.2.2 rdflib-sqlalchemy-0.3.8 tensor2tensor-1.10.0 tqdm-4.28.1\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip install --upgrade tensor2tensor gutenberg\n",
    "#git clone https://github.com/tensorflow/tensor2tensor.git\n",
    "#cd tensor2tensor; pip install --user -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a pip install, click **\"Reset Session\"** on the notebook so that the Python environment picks up the new packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh-tensorflow==0.0.3\n",
      "tensor2tensor==1.10.0\n",
      "tensorboard==1.8.0\n",
      "tensorflow==1.8.0\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-c56dd36db3fa27d6' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'qwiklabs-gcp-c56dd36db3fa27d6' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# this is what this notebook is demonstrating\n",
    "PROBLEM= 'poetry_line_problem'\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['PROBLEM'] = PROBLEM\n",
    "\n",
    "#os.environ['PATH'] = os.environ['PATH'] + ':' + os.getcwd() + '/tensor2tensor/tensor2tensor/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We will get some <a href=\"https://www.gutenberg.org/wiki/Poetry_(Bookshelf)\">poetry anthologies</a> from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf data/poetry\n",
    "mkdir -p data/poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote lines 0 to 2802 from Victorian songs\n",
      "Wrote lines 2802 to 7503 from Baldwin collection\n",
      "Wrote lines 7503 to 14069 from Swinburne collection\n",
      "Wrote lines 14069 to 14926 from Blake\n",
      "Wrote lines 14926 to 42441 from Bulchevys collection\n",
      "Wrote lines 42441 to 51789 from Palgrave-Pearse collection\n",
      "Wrote lines 51789 to 56298 from Knowles collection\n"
     ]
    }
   ],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import re\n",
    "\n",
    "books = [\n",
    "  # bookid, skip N lines\n",
    "  (26715, 1000, 'Victorian songs'),\n",
    "  (30235, 580, 'Baldwin collection'),\n",
    "  (35402, 710, 'Swinburne collection'),\n",
    "  (574, 15, 'Blake'),\n",
    "  (1304, 172, 'Bulchevys collection'),\n",
    "  (19221, 223, 'Palgrave-Pearse collection'),\n",
    "  (15553, 522, 'Knowles collection') \n",
    "]\n",
    "\n",
    "with open('data/poetry/raw.txt', 'w') as ofp:\n",
    "  lineno = 0\n",
    "  for (id_nr, toskip, title) in books:\n",
    "    startline = lineno\n",
    "    text = strip_headers(load_etext(id_nr)).strip()\n",
    "    lines = text.split('\\n')[toskip:]\n",
    "    # any line that is all upper case is a title or author name\n",
    "    # also don't want any lines with years (numbers)\n",
    "    for line in lines:\n",
    "      if (len(line) > 0 \n",
    "          and line.upper() != line \n",
    "          and not re.match('.*[0-9]+.*', line)\n",
    "          and len(line) < 50\n",
    "         ):\n",
    "        cleaned = re.sub('[^a-z\\'\\-]+', ' ', line.strip().lower())\n",
    "        ofp.write(cleaned)\n",
    "        ofp.write('\\n')\n",
    "        lineno = lineno + 1\n",
    "      else:\n",
    "        ofp.write('\\n')\n",
    "    print('Wrote lines {} to {} from {}'.format(startline, lineno, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88133 data/poetry/raw.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "We are going to train a machine learning model to write poetry given a starting point. We'll give it one line, and it is going to tell us the next line.  So, naturally, we will train it on real poetry. Our feature will be a line of a poem and the label will be next line of that poem.\n",
    "<p>\n",
    "Our training dataset will consist of two files.  The first file will consist of the input lines of poetry and the other file will consist of the corresponding output lines, one output line per input line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/poetry/raw.txt', 'r') as rawfp,\\\n",
    "  open('data/poetry/input.txt', 'w') as infp,\\\n",
    "  open('data/poetry/output.txt', 'w') as outfp:\n",
    "    \n",
    "    prev_line = ''\n",
    "    for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only\n",
    "        # on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:       \n",
    "            infp.write(prev_line + '\\n')\n",
    "            outfp.write(curr_line + '\\n')\n",
    "        prev_line = curr_line      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> data/poetry/input.txt <==\r\n",
      "i sat beside the streamlet\r\n",
      "i watched the water flow\r\n",
      "as we together watched it\r\n",
      "one little year ago\r\n",
      "the soft rain pattered on the leaves\r\n",
      "\r\n",
      "==> data/poetry/output.txt <==\r\n",
      "i watched the water flow\r\n",
      "as we together watched it\r\n",
      "one little year ago\r\n",
      "the soft rain pattered on the leaves\r\n",
      "the april grass was wet\r\n",
      "\r\n",
      "==> data/poetry/raw.txt <==\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to generate the data beforehand -- instead, we can have Tensor2Tensor create the training dataset for us. So, in the code below, I will use only data/poetry/raw.txt -- obviously, this allows us to productionize our model better.  Simply keep collecting raw data and generate the training/test data at the time of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up problem\n",
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf poetry\n",
    "mkdir -p poetry/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poetry/trainer/problem.py\n"
     ]
    }
   ],
   "source": [
    "%writefile poetry/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "\n",
    "@registry.register_problem\n",
    "class PoetryLineProblem(text_problems.Text2TextProblem):\n",
    "  \"\"\"Predict next line of poetry from the last line. From Gutenberg texts.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # generate_data will NOT shard the data into TRAIN and EVAL for us.\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def dataset_splits(self):\n",
    "    \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n",
    "    # 10% evaluation data\n",
    "    return [{\n",
    "        \"split\": problem.DatasetSplit.TRAIN,\n",
    "        \"shards\": 90,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 10,\n",
    "    }]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    with open('data/poetry/raw.txt', 'r') as rawfp:\n",
    "      prev_line = ''\n",
    "      for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only\n",
    "        # on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:       \n",
    "            yield {\n",
    "                \"inputs\": prev_line,\n",
    "                \"targets\": curr_line\n",
    "            }\n",
    "        prev_line = curr_line          \n",
    "\n",
    "\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "@registry.register_hparams\n",
    "def transformer_poetry():\n",
    "  hparams = transformer.transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.attention_dropout = 0.6\n",
    "  hparams.layer_prepostprocess_dropout = 0.6\n",
    "  hparams.learning_rate = 0.05\n",
    "  return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_poetry_range(rhp):\n",
    "  rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "  rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "  rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poetry/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poetry/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poetry/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poetry/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "  'tensor2tensor'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='poetry',\n",
    "    version='0.1',\n",
    "    author = 'Google',\n",
    "    author_email = 'training-feedback@cloud.google.com',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Poetry Line Problem',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch poetry/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poetry\n",
      "poetry/setup.py\n",
      "poetry/__init__.py\n",
      "poetry/trainer\n",
      "poetry/trainer/__init__.py\n",
      "poetry/trainer/problem.py\n"
     ]
    }
   ],
   "source": [
    "!find poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data \n",
    "\n",
    "Our problem (translation) requires the creation of text sequences from the training dataset.  This is done using t2t-datagen and the Problem defined in the previous section.\n",
    "\n",
    "(Ignore any runtime warnings about change in size of numpy.dtype. they are harmless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "INFO:tensorflow:Generating problems:\n",
      "    poetry:\n",
      "      * poetry_line_problem\n",
      "INFO:tensorflow:Generating data for poetry_line_problem.\n",
      "INFO:tensorflow:Generating vocab file: ./t2t_data/vocab.poetry_line_problem.8192.subwords\n",
      "INFO:tensorflow:Trying min_count 500\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 1189\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 676\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 704\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 702\n",
      "INFO:tensorflow:Trying min_count 250\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 2025\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 1097\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 1130\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 1129\n",
      "INFO:tensorflow:Trying min_count 125\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 3386\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 1756\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 1798\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 1799\n",
      "INFO:tensorflow:Trying min_count 62\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 5620\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 2771\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 2832\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 2823\n",
      "INFO:tensorflow:Trying min_count 31\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 9026\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 4270\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 4362\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 4363\n",
      "INFO:tensorflow:Trying min_count 15\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 14356\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 6615\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 6754\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 6739\n",
      "INFO:tensorflow:Trying min_count 7\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 22520\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 10075\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 10220\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 10191\n",
      "INFO:tensorflow:Trying min_count 11\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 17325\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 7831\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 7973\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 7957\n",
      "INFO:tensorflow:Trying min_count 9\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 19507\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 8776\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 8943\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 8938\n",
      "INFO:tensorflow:Trying min_count 10\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 18370\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 8289\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 8432\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 8425\n",
      "INFO:tensorflow:Generating case 0.\n",
      "INFO:tensorflow:Generated 44341 Examples\n",
      "INFO:tensorflow:Shuffling data...\n",
      "INFO:tensorflow:Data shuffled.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR/tmp\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poetry_line_problem-dev-00000-of-00010\r\n",
      "poetry_line_problem-dev-00001-of-00010\r\n",
      "poetry_line_problem-dev-00002-of-00010\r\n",
      "poetry_line_problem-dev-00003-of-00010\r\n",
      "poetry_line_problem-dev-00004-of-00010\r\n",
      "poetry_line_problem-dev-00005-of-00010\r\n",
      "poetry_line_problem-dev-00006-of-00010\r\n",
      "poetry_line_problem-dev-00007-of-00010\r\n",
      "poetry_line_problem-dev-00008-of-00010\r\n",
      "poetry_line_problem-dev-00009-of-00010\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls t2t_data | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Cloud ML Engine access to data\n",
    "\n",
    "Copy the data to Google Cloud Storage, and then provide access to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Copying file://./t2t_data/poetry_line_problem-dev-00000-of-00010 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-dev-00001-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 32.3 KiB]                                                \r",
      "/ [0 files][    0.0 B/ 64.6 KiB]                                                \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00002-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 97.0 KiB]                                                \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00003-of-00010 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-dev-00004-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/129.4 KiB]                                                \r",
      "/ [0 files][    0.0 B/161.9 KiB]                                                \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00005-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [1/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [1/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00006-of-00010 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-dev-00007-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [2/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [3/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [4/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [4/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [4/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00008-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [4/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "/ [5/101 files][161.9 KiB/  3.2 MiB]   4% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-dev-00009-of-00010 [Content-Type=application/octet-stream]...\n",
      "/ [5/101 files][194.1 KiB/  3.2 MiB]   5% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00000-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [6/101 files][323.6 KiB/  3.2 MiB]   9% Done                                  \r",
      "/ [6/101 files][323.6 KiB/  3.2 MiB]   9% Done                                  \r",
      "/ [7/101 files][356.3 KiB/  3.2 MiB]  10% Done                                  \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00001-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [7/101 files][356.3 KiB/  3.2 MiB]  10% Done                                  \r",
      "-\r",
      "Copying file://./t2t_data/poetry_line_problem-train-00002-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [8/101 files][388.8 KiB/  3.2 MiB]  11% Done                                  \r",
      "- [8/101 files][388.8 KiB/  3.2 MiB]  11% Done                                  \r",
      "- [9/101 files][421.2 KiB/  3.2 MiB]  12% Done                                  \r",
      "- [10/101 files][421.2 KiB/  3.2 MiB]  12% Done                                 \r",
      "- [11/101 files][421.2 KiB/  3.2 MiB]  12% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00003-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00004-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00005-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [11/101 files][421.2 KiB/  3.2 MiB]  12% Done                                 \r",
      "- [11/101 files][421.2 KiB/  3.2 MiB]  12% Done                                 \r",
      "- [11/101 files][421.2 KiB/  3.2 MiB]  12% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00006-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [12/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "- [12/101 files][518.8 KiB/  3.2 MiB]  15% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00007-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [13/101 files][551.3 KiB/  3.2 MiB]  16% Done                                 \r",
      "- [13/101 files][551.3 KiB/  3.2 MiB]  16% Done                                 \r",
      "- [14/101 files][583.8 KiB/  3.2 MiB]  17% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00008-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [14/101 files][583.8 KiB/  3.2 MiB]  17% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00009-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00010-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00011-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [15/101 files][616.3 KiB/  3.2 MiB]  18% Done                                 \r",
      "- [16/101 files][616.3 KiB/  3.2 MiB]  18% Done                                 \r",
      "- [17/101 files][616.3 KiB/  3.2 MiB]  18% Done                                 \r",
      "- [17/101 files][616.3 KiB/  3.2 MiB]  18% Done                                 \r",
      "- [17/101 files][616.3 KiB/  3.2 MiB]  18% Done                                 \r",
      "- [17/101 files][616.3 KiB/  3.2 MiB]  18% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00012-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [18/101 files][713.7 KiB/  3.2 MiB]  21% Done                                 \r",
      "- [18/101 files][713.7 KiB/  3.2 MiB]  21% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00013-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [19/101 files][746.1 KiB/  3.2 MiB]  22% Done                                 \r",
      "\\\r",
      "\\ [19/101 files][746.1 KiB/  3.2 MiB]  22% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00014-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [20/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "\\ [20/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "\\ [21/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "\\ [22/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00016-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00015-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [22/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "\\ [22/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "\\ [23/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00017-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [23/101 files][778.5 KiB/  3.2 MiB]  23% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00018-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00021-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00020-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00019-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [24/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "\\ [24/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "\\ [25/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "\\ [26/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "\\ [27/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "\\ [28/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "\\ [28/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00022-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [28/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "\\ [28/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "\\ [28/101 files][908.3 KiB/  3.2 MiB]  27% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00023-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00024-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00025-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [29/101 files][  1.1 MiB/  3.2 MiB]  32% Done                                 \r",
      "\\ [29/101 files][  1.1 MiB/  3.2 MiB]  32% Done                                 \r",
      "\\ [30/101 files][  1.1 MiB/  3.2 MiB]  32% Done                                 \r",
      "\\ [31/101 files][  1.1 MiB/  3.2 MiB]  32% Done                                 \r",
      "\\ [31/101 files][  1.1 MiB/  3.2 MiB]  32% Done                                 \r",
      "\\ [31/101 files][  1.1 MiB/  3.2 MiB]  32% Done                                 \r",
      "|\r",
      "Copying file://./t2t_data/poetry_line_problem-train-00026-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00027-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [32/101 files][  1.1 MiB/  3.2 MiB]  35% Done                                 \r",
      "| [32/101 files][  1.1 MiB/  3.2 MiB]  35% Done                                 \r",
      "| [33/101 files][  1.1 MiB/  3.2 MiB]  35% Done                                 \r",
      "| [33/101 files][  1.1 MiB/  3.2 MiB]  35% Done                                 \r",
      "| [34/101 files][  1.2 MiB/  3.2 MiB]  37% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00028-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [34/101 files][  1.2 MiB/  3.2 MiB]  37% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00029-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [35/101 files][  1.2 MiB/  3.2 MiB]  37% Done                                 \r",
      "| [35/101 files][  1.2 MiB/  3.2 MiB]  37% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00030-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [36/101 files][  1.3 MiB/  3.2 MiB]  39% Done                                 \r",
      "| [36/101 files][  1.3 MiB/  3.2 MiB]  39% Done                                 \r",
      "| [37/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00031-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [37/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00032-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [38/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "| [38/101 files][  1.3 MiB/  3.2 MiB]  40% Done                                 \r",
      "| [39/101 files][  1.4 MiB/  3.2 MiB]  42% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00033-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [39/101 files][  1.4 MiB/  3.2 MiB]  42% Done                                 \r",
      "| [40/101 files][  1.4 MiB/  3.2 MiB]  42% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00034-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [40/101 files][  1.4 MiB/  3.2 MiB]  42% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00035-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [41/101 files][  1.4 MiB/  3.2 MiB]  44% Done                                 \r",
      "| [41/101 files][  1.4 MiB/  3.2 MiB]  44% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00036-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [42/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "/\r",
      "/ [42/101 files][  1.5 MiB/  3.2 MiB]  45% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00037-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [43/101 files][  1.5 MiB/  3.2 MiB]  46% Done                                 \r",
      "/ [43/101 files][  1.5 MiB/  3.2 MiB]  46% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00038-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [44/101 files][  1.5 MiB/  3.2 MiB]  47% Done                                 \r",
      "/ [44/101 files][  1.5 MiB/  3.2 MiB]  47% Done                                 \r",
      "/ [45/101 files][  1.5 MiB/  3.2 MiB]  47% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00039-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [45/101 files][  1.5 MiB/  3.2 MiB]  47% Done                                 \r",
      "/ [46/101 files][  1.5 MiB/  3.2 MiB]  47% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00040-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [46/101 files][  1.5 MiB/  3.2 MiB]  47% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00041-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [47/101 files][  1.6 MiB/  3.2 MiB]  50% Done                                 \r",
      "/ [47/101 files][  1.6 MiB/  3.2 MiB]  50% Done                                 \r",
      "/ [48/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "/ [49/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "/ [50/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "/ [51/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00042-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [51/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00043-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [51/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00044-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [51/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00045-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [51/101 files][  1.7 MiB/  3.2 MiB]  51% Done                                 \r",
      "/ [52/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00046-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [52/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "/ [53/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "/ [54/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "/ [55/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "/ [56/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00047-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00050-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00049-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00048-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [56/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "/ [56/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "/ [56/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "/ [56/101 files][  1.8 MiB/  3.2 MiB]  54% Done                                 \r",
      "-\r",
      "Copying file://./t2t_data/poetry_line_problem-train-00051-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [57/101 files][  1.9 MiB/  3.2 MiB]  59% Done                                 \r",
      "- [57/101 files][  1.9 MiB/  3.2 MiB]  59% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00052-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [58/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "- [59/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00053-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [59/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "- [59/101 files][  2.0 MiB/  3.2 MiB]  60% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00055-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00054-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [60/101 files][  2.0 MiB/  3.2 MiB]  62% Done                                 \r",
      "- [61/101 files][  2.0 MiB/  3.2 MiB]  62% Done                                 \r",
      "- [61/101 files][  2.0 MiB/  3.2 MiB]  62% Done                                 \r",
      "- [61/101 files][  2.0 MiB/  3.2 MiB]  62% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00056-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [62/101 files][  2.0 MiB/  3.2 MiB]  62% Done                                 \r",
      "- [62/101 files][  2.0 MiB/  3.2 MiB]  62% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00057-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [63/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "- [63/101 files][  2.1 MiB/  3.2 MiB]  65% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00059-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00058-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00060-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [64/101 files][  2.2 MiB/  3.2 MiB]  66% Done                                 \r",
      "- [65/101 files][  2.2 MiB/  3.2 MiB]  66% Done                                 \r",
      "- [65/101 files][  2.2 MiB/  3.2 MiB]  66% Done                                 \r",
      "- [65/101 files][  2.2 MiB/  3.2 MiB]  66% Done                                 \r",
      "- [66/101 files][  2.2 MiB/  3.2 MiB]  66% Done                                 \r",
      "- [66/101 files][  2.2 MiB/  3.2 MiB]  66% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00061-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [67/101 files][  2.2 MiB/  3.2 MiB]  68% Done                                 \r",
      "- [67/101 files][  2.3 MiB/  3.2 MiB]  69% Done                                 \r",
      "\\\r",
      "Copying file://./t2t_data/poetry_line_problem-train-00062-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [68/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "\\ [68/101 files][  2.3 MiB/  3.2 MiB]  70% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00063-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [69/101 files][  2.3 MiB/  3.2 MiB]  71% Done                                 \r",
      "\\ [69/101 files][  2.3 MiB/  3.2 MiB]  71% Done                                 \r",
      "\\ [70/101 files][  2.3 MiB/  3.2 MiB]  72% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00064-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [70/101 files][  2.3 MiB/  3.2 MiB]  72% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00065-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [71/101 files][  2.4 MiB/  3.2 MiB]  73% Done                                 \r",
      "\\ [71/101 files][  2.4 MiB/  3.2 MiB]  73% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00066-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [72/101 files][  2.4 MiB/  3.2 MiB]  74% Done                                 \r",
      "\\ [72/101 files][  2.4 MiB/  3.2 MiB]  74% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00067-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [73/101 files][  2.4 MiB/  3.2 MiB]  75% Done                                 \r",
      "\\ [73/101 files][  2.4 MiB/  3.2 MiB]  75% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00068-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [74/101 files][  2.5 MiB/  3.2 MiB]  76% Done                                 \r",
      "\\ [74/101 files][  2.5 MiB/  3.2 MiB]  76% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00069-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [75/101 files][  2.5 MiB/  3.2 MiB]  77% Done                                 \r",
      "\\ [75/101 files][  2.5 MiB/  3.2 MiB]  77% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00070-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [76/101 files][  2.5 MiB/  3.2 MiB]  78% Done                                 \r",
      "\\ [76/101 files][  2.5 MiB/  3.2 MiB]  78% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00071-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00072-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00073-of-00090 [Content-Type=application/octet-stream]...\n",
      "\\ [77/101 files][  2.6 MiB/  3.2 MiB]  79% Done                                 \r",
      "\\ [77/101 files][  2.6 MiB/  3.2 MiB]  79% Done                                 \r",
      "\\ [78/101 files][  2.6 MiB/  3.2 MiB]  79% Done                                 \r",
      "\\ [78/101 files][  2.6 MiB/  3.2 MiB]  79% Done                                 \r",
      "\\ [79/101 files][  2.6 MiB/  3.2 MiB]  79% Done                                 \r",
      "\\ [79/101 files][  2.6 MiB/  3.2 MiB]  79% Done                                 \r",
      "|\r",
      "| [80/101 files][  2.6 MiB/  3.2 MiB]  80% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00074-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [80/101 files][  2.6 MiB/  3.2 MiB]  80% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00075-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [81/101 files][  2.6 MiB/  3.2 MiB]  81% Done                                 \r",
      "| [81/101 files][  2.6 MiB/  3.2 MiB]  81% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00076-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00077-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [82/101 files][  2.7 MiB/  3.2 MiB]  84% Done                                 \r",
      "| [82/101 files][  2.7 MiB/  3.2 MiB]  84% Done                                 \r",
      "| [83/101 files][  2.7 MiB/  3.2 MiB]  84% Done                                 \r",
      "| [83/101 files][  2.7 MiB/  3.2 MiB]  84% Done                                 \r",
      "| [84/101 files][  2.7 MiB/  3.2 MiB]  84% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00078-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [84/101 files][  2.8 MiB/  3.2 MiB]  85% Done                                 \r",
      "| [85/101 files][  2.8 MiB/  3.2 MiB]  86% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00079-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [85/101 files][  2.8 MiB/  3.2 MiB]  86% Done                                 \r",
      "| [86/101 files][  2.8 MiB/  3.2 MiB]  87% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00080-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [86/101 files][  2.8 MiB/  3.2 MiB]  87% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00081-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00082-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [87/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "| [88/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "| [88/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "| [88/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "| [89/101 files][  2.9 MiB/  3.2 MiB]  89% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00083-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [89/101 files][  2.9 MiB/  3.2 MiB]  90% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00084-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00085-of-00090 [Content-Type=application/octet-stream]...\n",
      "Copying file://./t2t_data/poetry_line_problem-train-00086-of-00090 [Content-Type=application/octet-stream]...\n",
      "| [90/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "| [91/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "/\r",
      "/ [91/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "/ [92/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "/ [93/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "/ [93/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00087-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [93/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "/ [93/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "/ [94/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00088-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [94/101 files][  3.0 MiB/  3.2 MiB]  92% Done                                 \r",
      "Copying file://./t2t_data/poetry_line_problem-train-00089-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [95/101 files][  3.1 MiB/  3.2 MiB]  96% Done                                 \r",
      "/ [95/101 files][  3.1 MiB/  3.2 MiB]  96% Done                                 \r",
      "Copying file://./t2t_data/vocab.poetry_line_problem.8192.subwords [Content-Type=application/octet-stream]...\n",
      "/ [96/101 files][  3.1 MiB/  3.2 MiB]  96% Done                                 \r",
      "/ [97/101 files][  3.1 MiB/  3.2 MiB]  96% Done                                 \r",
      "/ [97/101 files][  3.1 MiB/  3.2 MiB]  96% Done                                 \r",
      "/ [98/101 files][  3.1 MiB/  3.2 MiB]  96% Done                                 \r",
      "/ [99/101 files][  3.2 MiB/  3.2 MiB]  99% Done                                 \r",
      "/ [100/101 files][  3.2 MiB/  3.2 MiB]  99% Done                                \r",
      "/ [101/101 files][  3.2 MiB/  3.2 MiB] 100% Done                                \r\n",
      "Operation completed over 101 objects/3.2 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "DATA_DIR=./t2t_data\n",
    "gsutil -m rm -r gs://${BUCKET}/poetry/\n",
    "gsutil -m cp ${DATA_DIR}/${PROBLEM}* ${DATA_DIR}/vocab* gs://${BUCKET}/poetry/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorizing the Cloud ML Service account service-583952583475@cloud-ml.google.com.iam.gserviceaccount.com to access files in qwiklabs-gcp-c56dd36db3fa27d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   235    0   235    0     0    929      0 --:--:-- --:--:-- --:--:--   932\n",
      "Updated default ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/datalab-backups/us-central1-a/mydatalabvm/content/daily-20181107103255\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/datalab-backups/us-central1-a/mydatalabvm/content/hourly-20181107103255\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00002-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00003-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00000-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00001-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00004-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/datalab-backups/us-central1-a/mydatalabvm/content/weekly-20181107103255\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00005-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00006-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00007-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00008-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00009-of-00010\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00000-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00001-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00004-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00002-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00005-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00006-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00003-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00007-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00008-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00009-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00010-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00013-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00012-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00014-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00011-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00015-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00017-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00016-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00019-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00018-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00020-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00022-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00024-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00021-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00023-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00025-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00026-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00027-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00028-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00030-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00029-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00031-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00032-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00033-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00034-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00036-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00038-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00037-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00039-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00035-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00040-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00041-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00044-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00043-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00042-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00045-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00046-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00047-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00049-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00048-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00050-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00051-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00052-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00055-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00056-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00054-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00053-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00057-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00059-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00058-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00060-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00062-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00061-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00064-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00065-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00063-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00066-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00067-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00068-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00069-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00070-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00071-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00072-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00073-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00074-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00075-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00076-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00077-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00079-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00078-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00080-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00081-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00082-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00083-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00084-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00085-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00088-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/vocab.poetry_line_problem.8192.subwords\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00086-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00087-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00089-of-00090\n",
      "Updated ACL on gs://qwiklabs-gcp-c56dd36db3fa27d6/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print(response['serviceAccount'])\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model locally\n",
    "\n",
    "Let's run it locally on a subset of the data to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00080-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 32.4 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00081-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 64.8 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00082-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 97.1 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00083-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/129.5 KiB]                                                \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00084-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/161.9 KiB]                                                \r",
      "/ [1/12 files][ 32.4 KiB/425.2 KiB]   7% Done                                   \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00085-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [2/12 files][ 64.7 KiB/425.2 KiB]  15% Done                                   \r",
      "/ [2/12 files][ 97.1 KiB/425.2 KiB]  22% Done                                   \r",
      "/ [3/12 files][129.4 KiB/425.2 KiB]  30% Done                                   \r",
      "/ [4/12 files][129.4 KiB/425.2 KiB]  30% Done                                   \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00086-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [4/12 files][129.4 KiB/425.2 KiB]  30% Done                                   \r",
      "/ [5/12 files][161.9 KiB/425.2 KiB]  38% Done                                   \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00087-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [5/12 files][161.9 KiB/425.2 KiB]  38% Done                                   \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00088-of-00090 [Content-Type=application/octet-stream]...\n",
      "/ [5/12 files][161.9 KiB/425.2 KiB]  38% Done                                   \r",
      "-\r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-train-00089-of-00090 [Content-Type=application/octet-stream]...\n",
      "- [5/12 files][161.9 KiB/425.2 KiB]  38% Done                                   \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/poetry_line_problem-dev-00000-of-00010 [Content-Type=application/octet-stream]...\n",
      "- [6/12 files][194.2 KiB/425.2 KiB]  45% Done                                   \r",
      "- [6/12 files][194.2 KiB/425.2 KiB]  45% Done                                   \r",
      "Copying gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/data/vocab.poetry_line_problem.8192.subwords [Content-Type=application/octet-stream]...\n",
      "- [7/12 files][226.6 KiB/425.2 KiB]  53% Done                                   \r",
      "- [8/12 files][323.8 KiB/425.2 KiB]  76% Done                                   \r",
      "- [9/12 files][323.8 KiB/425.2 KiB]  76% Done                                   \r",
      "- [10/12 files][323.8 KiB/425.2 KiB]  76% Done                                  \r",
      "- [10/12 files][323.8 KiB/425.2 KiB]  76% Done                                  \r",
      "\\\r",
      "\\ [11/12 files][393.0 KiB/425.2 KiB]  92% Done                                  \r",
      "\\ [12/12 files][425.2 KiB/425.2 KiB] 100% Done                                  \r\n",
      "Operation completed over 12 objects/425.2 KiB.                                   \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "BASE=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/subset\n",
    "gsutil -m rm -r $OUTDIR\n",
    "gsutil -m cp \\\n",
    "    ${BASE}/${PROBLEM}-train-0008* \\\n",
    "    ${BASE}/${PROBLEM}-dev-00000*  \\\n",
    "    ${BASE}/vocab* \\\n",
    "    $OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the following will work only if you are running Datalab on a reasonably powerful machine. Don't be alarmed if your process is killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::MLPv0.5.0 transformer 1541588238.846117020 (/usr/local/envs/py2env/bin/t2t-trainer:28) run_start\n",
      ":::MLPv0.5.0 transformer 1541588238.847059965 (/usr/local/envs/py2env/bin/t2t-trainer:28) run_set_random_seed\n",
      ":::MLPv0.5.0 transformer 1541588239.336826086 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/data_generators/problem.py:869) input_order\n",
      ":::MLPv0.5.0 transformer 1541588240.869167089 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py:213) model_hp_initializer_gain: 1.0\n",
      ":::MLPv0.5.0 transformer 1541588241.239887953 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:186) model_hp_layer_postprocess_dropout: 0.6\n",
      ":::MLPv0.5.0 transformer 1541588241.253107071 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_hidden_layers: 2\n",
      ":::MLPv0.5.0 transformer 1541588241.254679918 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_attention_num_heads: 4\n",
      ":::MLPv0.5.0 transformer 1541588241.256187916 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_attention_dropout: 0.6\n",
      ":::MLPv0.5.0 transformer 1541588241.697252989 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588241.699033022 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588241.700582027 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_relu_dropout: 0.1\n",
      ":::MLPv0.5.0 transformer 1541588242.477513075 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588242.479209900 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588242.480720043 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_relu_dropout: 0.1\n",
      ":::MLPv0.5.0 transformer 1541588242.650954962 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_norm: {\"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588242.808770895 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:202) model_hp_layer_postprocess_dropout: 0.6\n",
      ":::MLPv0.5.0 transformer 1541588242.821542978 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_hidden_layers: 2\n",
      ":::MLPv0.5.0 transformer 1541588242.823122025 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_attention_num_heads: 4\n",
      ":::MLPv0.5.0 transformer 1541588242.825001001 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_attention_dropout: 0.6\n",
      ":::MLPv0.5.0 transformer 1541588243.614659071 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588243.616313934 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588243.617830992 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_relu_dropout: 0.1\n",
      ":::MLPv0.5.0 transformer 1541588244.835025072 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588244.836659908 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588244.838198900 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_relu_dropout: 0.1\n",
      ":::MLPv0.5.0 transformer 1541588244.982469082 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_norm: {\"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588245.191807985 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py:45) opt_name: \"Adam\"\n",
      ":::MLPv0.5.0 transformer 1541588245.193130970 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py:45) opt_hp_Adam_beta1: 0.9\n",
      ":::MLPv0.5.0 transformer 1541588245.194530964 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py:45) opt_hp_Adam_beta2: 0.997\n",
      ":::MLPv0.5.0 transformer 1541588245.195804119 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py:45) opt_hp_Adam_epsilon: 1e-09\n",
      ":::MLPv0.5.0 transformer 1541588398.696475983 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py:213) model_hp_initializer_gain: 1.0\n",
      ":::MLPv0.5.0 transformer 1541588399.095529079 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:186) model_hp_layer_postprocess_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588399.098603010 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_hidden_layers: 2\n",
      ":::MLPv0.5.0 transformer 1541588399.100198984 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_attention_num_heads: 4\n",
      ":::MLPv0.5.0 transformer 1541588399.101720095 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_attention_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588399.491317987 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588399.493081093 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588399.494621992 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588400.734895945 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588400.736757040 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588400.738321066 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588400.887099028 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_norm: {\"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588401.015697002 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:202) model_hp_layer_postprocess_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588401.018290997 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_hidden_layers: 2\n",
      ":::MLPv0.5.0 transformer 1541588401.019840956 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_attention_num_heads: 4\n",
      ":::MLPv0.5.0 transformer 1541588401.021312952 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_attention_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588401.828527927 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588401.830245018 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588401.831782103 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588402.972254992 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588402.973963022 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588402.975522995 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588403.098380089 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_norm: {\"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588416.798924923 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py:213) model_hp_initializer_gain: 1.0\n",
      ":::MLPv0.5.0 transformer 1541588417.565844059 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:186) model_hp_layer_postprocess_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588417.568483114 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_hidden_layers: 2\n",
      ":::MLPv0.5.0 transformer 1541588417.570391893 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_attention_num_heads: 4\n",
      ":::MLPv0.5.0 transformer 1541588417.571770906 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_attention_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588418.013413906 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588418.014941931 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588418.016310930 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588418.531548023 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588418.533117056 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588418.534584999 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588418.676278114 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_norm: {\"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588418.862334967 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:202) model_hp_layer_postprocess_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588418.864731073 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_hidden_layers: 2\n",
      ":::MLPv0.5.0 transformer 1541588418.866126060 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_attention_num_heads: 4\n",
      ":::MLPv0.5.0 transformer 1541588418.867428064 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_attention_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588419.851237059 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588419.852777958 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588419.854196072 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588420.756647110 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541588420.758213043 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588420.764707088 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541588420.889089108 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_norm: {\"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541588431.515575886 (/usr/local/envs/py2env/bin/t2t-trainer:28) run_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py:230: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:Configuring DataParallelism to replicate the model.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 20, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4317a1f910>, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", 'use_tpu': False, '_tf_random_seed': None, '_num_worker_replicas': 0, '_task_id': 0, 't2t_device_info': {'num_async_replicas': 1}, '_evaluation_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_train_distribute': None, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_environment': 'local', '_master': '', '_model_dir': './trained_model', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f4317a1f950>, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7f43179b52a8>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:ValidationMonitor only works with --schedule=train_and_evaluate\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Reading data files from gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/subset/poetry_line_problem-train*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 10\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8425_128.bottom\n",
      "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_8425_128.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8425_128.top\n",
      "INFO:tensorflow:Base learning rate: 2.000000\n",
      "INFO:tensorflow:Trainable Variables Total size: 2005632\n",
      "INFO:tensorflow:Using optimizer Adam\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2018-11-07 10:57:59.543581: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:94] Filling up shuffle buffer (this may take a while): 102 of 512\n",
      "2018-11-07 10:58:09.483158: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:94] Filling up shuffle buffer (this may take a while): 233 of 512\n",
      "2018-11-07 10:58:19.428517: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:94] Filling up shuffle buffer (this may take a while): 368 of 512\n",
      "2018-11-07 10:58:29.513103: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:94] Filling up shuffle buffer (this may take a while): 504 of 512\n",
      "2018-11-07 10:58:30.023778: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:129] Shuffle buffer filled.\n",
      "2018-11-07 10:58:31.647875: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2018-11-07 10:58:32.009026: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.35086, step = 1\n",
      "2018-11-07 10:58:45.095894: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2018-11-07 10:58:45.396746: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2018-11-07 10:58:52.275799: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2018-11-07 10:58:52.629748: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:00.664487: W tensorflow/core/framework/allocator.cc:101] Allocation of 116770500 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:01.080247: W tensorflow/core/framework/allocator.cc:101] Allocation of 116770500 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:09.397408: W tensorflow/core/framework/allocator.cc:101] Allocation of 106155000 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:09.788796: W tensorflow/core/framework/allocator.cc:101] Allocation of 106155000 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:17.604132: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:17.979277: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:25.052239: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:25.428001: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:33.528193: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:33.956216: W tensorflow/core/framework/allocator.cc:101] Allocation of 113232000 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:41.983136: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:42.282963: W tensorflow/core/framework/allocator.cc:101] Allocation of 95539500 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:49.305607: W tensorflow/core/framework/allocator.cc:101] Allocation of 106155000 exceeds 10% of system memory.\n",
      "2018-11-07 10:59:49.657106: W tensorflow/core/framework/allocator.cc:101] Allocation of 106155000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Saving checkpoints for 10 into ./trained_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.307862.\n",
      "INFO:tensorflow:Reading data files from gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/subset/poetry_line_problem-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8425_128.bottom\n",
      "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_8425_128.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8425_128.top\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-07-11:00:04\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2018-11-07 11:00:08.425365: W tensorflow/core/framework/allocator.cc:101] Allocation of 23185600 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:08.506356: W tensorflow/core/framework/allocator.cc:101] Allocation of 23185600 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:08.675498: W tensorflow/core/framework/allocator.cc:101] Allocation of 23185600 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:09.738073: W tensorflow/core/framework/allocator.cc:101] Allocation of 31543200 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:09.853213: W tensorflow/core/framework/allocator.cc:101] Allocation of 31543200 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:10.075230: W tensorflow/core/framework/allocator.cc:101] Allocation of 31543200 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:11.333600: W tensorflow/core/framework/allocator.cc:101] Allocation of 29656000 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:11.436954: W tensorflow/core/framework/allocator.cc:101] Allocation of 29656000 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:11.647770: W tensorflow/core/framework/allocator.cc:101] Allocation of 29656000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-07-11:00:14\n",
      "INFO:tensorflow:Saving dict for global step 10: global_step = 10, loss = 9.711894, metrics-poetry_line_problem/targets/accuracy = 0.0013612851, metrics-poetry_line_problem/targets/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/targets/accuracy_top5 = 0.001633542, metrics-poetry_line_problem/targets/approx_bleu_score = 0.72313553, metrics-poetry_line_problem/targets/neg_log_perplexity = -9.699321, metrics-poetry_line_problem/targets/rouge_2_fscore = 0.76264536, metrics-poetry_line_problem/targets/rouge_L_fscore = 0.8049414\n",
      "INFO:tensorflow:Reading data files from gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/subset/poetry_line_problem-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8425_128.bottom\n",
      "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_8425_128.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8425_128.top\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-07-11:00:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2018-11-07 11:00:25.571301: W tensorflow/core/framework/allocator.cc:101] Allocation of 23185600 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:25.646841: W tensorflow/core/framework/allocator.cc:101] Allocation of 23185600 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:25.857942: W tensorflow/core/framework/allocator.cc:101] Allocation of 23185600 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:26.843301: W tensorflow/core/framework/allocator.cc:101] Allocation of 31543200 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:26.940784: W tensorflow/core/framework/allocator.cc:101] Allocation of 31543200 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:27.162070: W tensorflow/core/framework/allocator.cc:101] Allocation of 31543200 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:28.374790: W tensorflow/core/framework/allocator.cc:101] Allocation of 29656000 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:28.477905: W tensorflow/core/framework/allocator.cc:101] Allocation of 29656000 exceeds 10% of system memory.\n",
      "2018-11-07 11:00:28.720119: W tensorflow/core/framework/allocator.cc:101] Allocation of 29656000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-07-11:00:31\n",
      "INFO:tensorflow:Saving dict for global step 10: global_step = 10, loss = 9.711894, metrics-poetry_line_problem/targets/accuracy = 0.0013612851, metrics-poetry_line_problem/targets/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/targets/accuracy_top5 = 0.001633542, metrics-poetry_line_problem/targets/approx_bleu_score = 0.72313553, metrics-poetry_line_problem/targets/neg_log_perplexity = -9.699321, metrics-poetry_line_problem/targets/rouge_2_fscore = 0.76264536, metrics-poetry_line_problem/targets/rouge_L_fscore = 0.8049414\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "DATA_DIR=gs://${BUCKET}/poetry/subset\n",
    "OUTDIR=./trained_model\n",
    "rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR --job-dir=$OUTDIR --train_steps=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Cloud ML Engine\n",
    "\n",
    "tensor2tensor has a convenient --cloud_mlengine option to kick off the training on the managed service.\n",
    "It uses the [Python API](https://cloud.google.com/ml-engine/docs/training-jobs) mentioned in the Cloud ML Engine docs, rather than requiring you to use gcloud to submit the job.\n",
    "<p>\n",
    "Note: your project needs P100 quota in the region.\n",
    "<p>\n",
    "The echo is because t2t-trainer asks you to confirm before submitting the job to the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model us-central1 poetry_181107_111928\n",
      ":::MLPv0.5.0 transformer 1541589576.773981094 (/usr/local/envs/py2env/bin/t2t-trainer:28) run_start\n",
      ":::MLPv0.5.0 transformer 1541589576.774827957 (/usr/local/envs/py2env/bin/t2t-trainer:28) run_set_random_seed\n",
      "Confirm (Y/n)? > "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "INFO:tensorflow:Launching job transformer_poetry_line_problem_t2t_20181107_111937 with ML Engine spec:\n",
      "{'trainingInput': {'jobDir': 'gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model', 'runtimeVersion': '1.9', 'scaleTier': 'CUSTOM', 'masterType': 'standard_p100', 'region': u'us-central1', 'args': ['--eval_steps=100', '--worker_gpu_memory_fraction=0.95', '--ps_job=/job:ps', '--decode_hparams=', '--sync=False', '--optionally_use_dist_strat=False', '--eval_use_test_set=False', '--worker_id=0', '--eval_early_stopping_metric_minimize=True', '--worker_replicas=1', '--train_steps=7500', '--cloud_tpu_name=None-tpu', '--disable_ffmpeg=False', '--registry_help=False', '--eval_throttle_seconds=600', '--worker_gpu=1', '--keep_checkpoint_max=20', '--save_checkpoints_secs=0', '--gpu_order=', '--log_step_count_steps=100', '--master=', '--generate_data=False', '--intra_op_parallelism_threads=0', '--enable_graph_rewriter=False', '--export_saved_model=False', '--output_dir=gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model', '--profile=False', '--std_server_protocol=grpc', '--tmp_dir=/tmp/t2t_datagen', '--schedule=continuous_train_and_eval', '--inter_op_parallelism_threads=0', '--iterations_per_loop=100', '--hparams=', '--use_tpu=False', '--eval_early_stopping_metric_delta=0.1', '--eval_run_autoregressive=False', '--tfdbg=False', '--local_eval_frequency=1000', '--data_dir=gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/subset', '--ps_replicas=0', '--use_tpu_estimator=False', '--parsing_path=', '--problem=poetry_line_problem', '--log_device_placement=False', '--hparams_set=transformer_poetry', '--dbgprofile=False', '--timit_paths=', '--ps_gpu=0', '--tpu_num_shards=8', '--locally_shard_to_cpu=False', '--xla_compile=False', '--worker_job=/job:localhost', '--model=transformer', '--keep_checkpoint_every_n_hours=10000', '--eval_early_stopping_metric=loss'], 'pythonModule': 'tensor2tensor.bin.t2t_trainer', 'pythonVersion': '2.7'}, 'jobId': 'transformer_poetry_line_problem_t2t_20181107_111937'}\n",
      "INFO:tensorflow:Tarring and pushing local Tensor2Tensor package.\n",
      "INFO:tensorflow:Found PyPI T2T installation. Launching tensor2tensor==1.10.0\n",
      "Copying file:///tmp/tensor2tensor_tmp.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [0 files][    0.0 B/  271.0 B]                                                \r",
      "/ [1 files][  271.0 B/  271.0 B]                                                \r\n",
      "Operation completed over 1 objects/271.0 B.                                      \n",
      "INFO:tensorflow:Tarring and pushing t2t_usr_dir.\n",
      "Copying file:///tmp/t2t_usr_container.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [0 files][    0.0 B/  2.6 KiB]                                                \r",
      "/ [1 files][  2.6 KiB/  2.6 KiB]                                                \r\n",
      "Operation completed over 1 objects/2.6 KiB.                                      \n",
      "INFO:tensorflow:Launched transformer_poetry_line_problem_t2t_20181107_111937. See console to track: https://console.cloud.google.com/mlengine/jobs/.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# use one of these\n",
    "GPU=\"--train_steps=7500 --cloud_mlengine --worker_gpu=1\"\n",
    "TPU=\"--train_steps=25000 --use_tpu=True --cloud_tpu_name=poetrytpu\"\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"\\\"Y\\\"\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  ${GPU}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job took about <b>25 minutes</b> for me and ended with these evaluation metrics:\n",
    "<pre>\n",
    "Saving dict for global step 8000: global_step = 8000, loss = 6.03338, metrics-poetry_line_problem/accuracy = 0.138544, metrics-poetry_line_problem/accuracy_per_sequence = 0.0, metrics-poetry_line_problem/accuracy_top5 = 0.232037, metrics-poetry_line_problem/approx_bleu_score = 0.00492648, metrics-poetry_line_problem/neg_log_perplexity = -6.68994, metrics-poetry_line_problem/rouge_2_fscore = 0.00256089, metrics-poetry_line_problem/rouge_L_fscore = 0.128194\n",
    "</pre>\n",
    "Notice that accuracy_per_sequence is 0 -- Considering that we are asking the NN to be rather creative, that doesn't surprise me. Why am I looking at accuracy_per_sequence and not the other metrics? This is because it is more appropriate for problem we are solving; metrics like Bleu score are better for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model/t2t_usr_container.tar.gz\n",
      "gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model/tensor2tensor_tmp.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/poetry/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training longer\n",
    "\n",
    "Let's train on 4 GPUs for 75,000 steps. Note the change in the last line of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "\n",
    "XXX This takes 3 hours on 4 GPUs. Remove this line if you are sure you want to do this.\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_full2\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"Y\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --train_steps=75000 --cloud_mlengine --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job took <b>12 hours</b> for me and ended with these metrics:\n",
    "<pre>\n",
    "global_step = 76000, loss = 4.99763, metrics-poetry_line_problem/accuracy = 0.219792, metrics-poetry_line_problem/accuracy_per_sequence = 0.0192308, metrics-poetry_line_problem/accuracy_top5 = 0.37618, metrics-poetry_line_problem/approx_bleu_score = 0.017955, metrics-poetry_line_problem/neg_log_perplexity = -5.38725, metrics-poetry_line_problem/rouge_2_fscore = 0.0325563, metrics-poetry_line_problem/rouge_L_fscore = 0.210618\n",
    "</pre>\n",
    "At least the accuracy per sequence is no longer zero. It is now 0.0192308 ... note that we are using a relatively small dataset (12K lines) and this is *tiny* in the world of natural language problems.\n",
    "<p>\n",
    "In order that you have your expectations set correctly: a high-performing translation model needs 400-million lines of input and takes 1 whole day on a TPU pod!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-predict\n",
    "\n",
    "How will our poetry model do when faced with Rumi's spiritual couplets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/poetry/rumi.txt\n"
     ]
    }
   ],
   "source": [
    "%writefile data/poetry/rumi.txt\n",
    "Where did the handsome beloved go?\n",
    "I wonder, where did that tall, shapely cypress tree go?\n",
    "He spread his light among us like a candle.\n",
    "Where did he go? So strange, where did he go without me?\n",
    "All day long my heart trembles like a leaf.\n",
    "All alone at midnight, where did that beloved go?\n",
    "Go to the road, and ask any passing traveler — \n",
    "That soul-stirring companion, where did he go?\n",
    "Go to the garden, and ask the gardener — \n",
    "That tall, shapely rose stem, where did he go?\n",
    "Go to the rooftop, and ask the watchman — \n",
    "That unique sultan, where did he go?\n",
    "Like a madman, I search in the meadows!\n",
    "That deer in the meadows, where did he go?\n",
    "My tearful eyes overflow like a river — \n",
    "That pearl in the vast sea, where did he go?\n",
    "All night long, I implore both moon and Venus — \n",
    "That lovely face, like a moon, where did he go?\n",
    "If he is mine, why is he with others?\n",
    "Since he’s not here, to what “there” did he go?\n",
    "If his heart and soul are joined with God,\n",
    "And he left this realm of earth and water, where did he go?\n",
    "Tell me clearly, Shams of Tabriz,\n",
    "Of whom it is said, “The sun never dies” — where did he go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out the odd-numbered lines. We'll compare how close our model can get to the beauty of Rumi's second lines given his first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where did the handsome beloved go\n",
      "he spread his light among us like a candle\n",
      "all day long my heart trembles like a leaf\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "awk 'NR % 2 == 1' data/poetry/rumi.txt | tr '[:upper:]' '[:lower:]' | sed \"s/[^a-z\\'-\\ ]//g\" > data/poetry/rumi_leads.txt\n",
    "head -3 data/poetry/rumi_leads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::MLPv0.5.0 transformer 1541590884.964530945 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/expert_utils.py:231) model_hp_layer_postprocess_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541590884.967498064 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_hidden_layers: 2\n",
      ":::MLPv0.5.0 transformer 1541590884.968872070 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_attention_num_heads: 4\n",
      ":::MLPv0.5.0 transformer 1541590884.970118999 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_attention_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541590885.433244944 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541590885.434639931 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541590885.435918093 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541590885.966557026 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541590885.967933893 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541590885.969183922 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/layers/transformer_layers.py:182) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541590886.114695072 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:101) model_hp_norm: {\"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541590887.298403025 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/expert_utils.py:231) model_hp_layer_postprocess_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541590887.301364899 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_hidden_layers: 2\n",
      ":::MLPv0.5.0 transformer 1541590887.303114891 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_attention_num_heads: 4\n",
      ":::MLPv0.5.0 transformer 1541590887.304733992 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_attention_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541590887.925614119 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541590887.927397966 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541590887.929296017 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541590888.902179956 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_filter: {\"filter_size\": 512, \"activation\": \"relu\", \"use_bias\": \"True\"}\n",
      ":::MLPv0.5.0 transformer 1541590888.903966904 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_ffn_dense: {\"use_bias\": \"True\", \"hidden_size\": 128}\n",
      ":::MLPv0.5.0 transformer 1541590888.905636072 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:1290) model_hp_relu_dropout: 0.0\n",
      ":::MLPv0.5.0 transformer 1541590889.041800022 (/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/models/transformer.py:153) model_hp_norm: {\"hidden_size\": 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py:230: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:Configuring DataParallelism to replicate the model.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 20, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4a63f36390>, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", 'use_tpu': False, '_tf_random_seed': None, '_num_worker_replicas': 0, '_task_id': 0, 't2t_device_info': {'num_async_replicas': 1}, '_evaluation_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_train_distribute': None, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_environment': 'local', '_master': '', '_model_dir': 'gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f4a63f36250>, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7f4a62f9f0c8>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:decode_hp.batch_size not specified; default=32\n",
      "INFO:tensorflow:Performing decoding from file (data/poetry/rumi_leads.txt).\n",
      "INFO:tensorflow:Getting sorted inputs\n",
      "INFO:tensorflow: batch 1\n",
      "INFO:tensorflow:Decoding batch 0\n",
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'infer'\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Beam Decoding with beam size 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model/model.ckpt-7500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference results INPUT: if his heart and soul are joined with god\n",
      "INFO:tensorflow:Inference results OUTPUT: and ready harbinger\n",
      "INFO:tensorflow:Inference results INPUT: if he is mine why is he with others\n",
      "INFO:tensorflow:Inference results OUTPUT: to show mean mean mean\n",
      "INFO:tensorflow:Inference results INPUT: all night long i implore both moon and venus\n",
      "INFO:tensorflow:Inference results OUTPUT: if she will not seemely\n",
      "INFO:tensorflow:Inference results INPUT: go to the road and ask any passing traveler\n",
      "INFO:tensorflow:Inference results OUTPUT: the wand'd agonies\n",
      "INFO:tensorflow:Inference results INPUT: all day long my heart trembles like a leaf\n",
      "INFO:tensorflow:Inference results OUTPUT: the fishing organ shook the skies\n",
      "INFO:tensorflow:Inference results INPUT: he spread his light among us like a candle\n",
      "INFO:tensorflow:Inference results OUTPUT: and yet anon repairs of oars\n",
      "INFO:tensorflow:Inference results INPUT: like a madman i search in the meadows\n",
      "INFO:tensorflow:Inference results OUTPUT: and many a hey nonny nonny nonny nonny\n",
      "INFO:tensorflow:Inference results INPUT: go to the rooftop and ask the watchman\n",
      "INFO:tensorflow:Inference results OUTPUT: and gentlemen of spain bewtie\n",
      "INFO:tensorflow:Inference results INPUT: go to the garden and ask the gardener\n",
      "INFO:tensorflow:Inference results OUTPUT: to school along the public public public public way\n",
      "INFO:tensorflow:Inference results INPUT: my tearful eyes overflow like a river\n",
      "INFO:tensorflow:Inference results OUTPUT: and on the road\n",
      "INFO:tensorflow:Inference results INPUT: tell me clearly shams of tabriz\n",
      "INFO:tensorflow:Inference results OUTPUT: ofter times in disport\n",
      "INFO:tensorflow:Inference results INPUT: where did the handsome beloved go\n",
      "INFO:tensorflow:Inference results OUTPUT: and all things that charming lay\n",
      "INFO:tensorflow:Elapsed Time: 11.32160\n",
      "INFO:tensorflow:Averaged Single Token Generation Time: 0.0241709 (time 11.3119903 count 468)\n",
      "INFO:tensorflow:Writing decodes into data/poetry/rumi_leads.txt.transformer.transformer_poetry.poetry_line_problem.beam4.alpha0.6.decodes\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# same as the above training job ...\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model #_full2  # or ${TOPDIR}/poetry/model_full\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note </b> if you get an error about \"AttributeError: 'HParams' object has no attribute 'problems'\" please <b>Reset Session</b>, run the cell that defines the PROBLEM and run the above cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and all things that charming lay\n",
      "and yet anon repairs of oars\n",
      "the fishing organ shook the skies\n",
      "the wand'd agonies\n",
      "to school along the public public public public way\n",
      "and gentlemen of spain bewtie\n",
      "and many a hey nonny nonny nonny nonny\n",
      "and on the road\n",
      "if she will not seemely\n",
      "to show mean mean mean\n",
      "and ready harbinger\n",
      "ofter times in disport\n"
     ]
    }
   ],
   "source": [
    "%bash  \n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are still phrases and not complete sentences. This indicates that we might need to train longer or better somehow. We need to diagnose the model ...\n",
    "<p>\n",
    "### Diagnosing training run\n",
    "<p>\n",
    "Let's diagnose the training run to see what we'd improve the next time around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 19400. Click <a href=\"/_proxy/52295/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19400"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/poetry/model_full'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped TensorBoard with pid 19235\n"
     ]
    }
   ],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "    TensorBoard().stop(pid)\n",
    "    print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"diagrams/poetry_loss.png\"/></td>\n",
    "<td><img src=\"diagrams/poetry_acc.png\"/></td>\n",
    "</table>\n",
    "Looking at the loss curve, it is clear that we are overfitting (note that the orange training curve is well below the blue eval curve). Both loss curves and the accuracy-per-sequence curve, which is our key evaluation measure, plateaus after 40k. (The red curve is a faster way of computing the evaluation metric, and can be ignored). So, how do we improve the model? Well, we need to reduce overfitting and make sure the eval metrics keep going down as long as the loss is also going down.\n",
    "<p>\n",
    "What we really need to do is to get more data, but if that's not an option, we could try to reduce the NN and increase the dropout regularization. We could also do hyperparameter tuning on the dropout and network sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "tensor2tensor also supports hyperparameter tuning on Cloud ML Engine. Note the addition of the autotune flags.\n",
    "<p>\n",
    "The `transformer_poetry_range` was registered in problem.py above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "\n",
    "XXX This takes about 15 hours and consumes about 420 ML units.  Uncomment if you wish to proceed anyway\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_hparam\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"Y\" | t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_poetry \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --hparams_range=transformer_poetry_range \\\n",
    "  --autotune_objective='metrics-poetry_line_problem/accuracy_per_sequence' \\\n",
    "  --autotune_maximize \\\n",
    "  --autotune_max_trials=4 \\\n",
    "  --autotune_parallel_trials=4 \\\n",
    "  --train_steps=7500 --cloud_mlengine --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the above job, it took about 15 hours and finished with these as the best parameters:\n",
    "<pre>\n",
    "{\n",
    "      \"trialId\": \"37\",\n",
    "      \"hyperparameters\": {\n",
    "        \"hp_num_hidden_layers\": \"4\",\n",
    "        \"hp_learning_rate\": \"0.026711152525921437\",\n",
    "        \"hp_hidden_size\": \"512\",\n",
    "        \"hp_attention_dropout\": \"0.60589466163419292\"\n",
    "      },\n",
    "      \"finalMetric\": {\n",
    "        \"trainingStep\": \"8000\",\n",
    "        \"objectiveValue\": 0.0276162791997\n",
    "      }\n",
    "</pre>\n",
    "In other words, the accuracy per sequence achieved was 0.027 (as compared to 0.019 before hyperparameter tuning, so a <b>40% improvement!</b>) using 4 hidden layers, a learning rate of 0.0267, a hidden size of 512 and droput probability of 0.606. This is inspite of training for only 7500 steps instead of 75,000 steps ... we could train for 75k steps with these parameters, but I'll leave that as an exercise for you.\n",
    "<p>\n",
    "Instead, let's try predicting with this optimized model. Note the addition of the hp* flags in order to override the values hardcoded in the source code. (there is no need to specify learning rate and dropout because they are not used during inference). I am using 37 because I got the best result at trialId=37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/poetry\n",
      "INFO:tensorflow:Overriding hparams in transformer_poetry with num_hidden_layers=4,hidden_size=512\n",
      "WARNING:tensorflow:From /usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py:230: __init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:Configuring DataParallelism to replicate the model.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 20, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc14bca0390>, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", 'use_tpu': False, '_tf_random_seed': None, '_num_worker_replicas': 0, '_task_id': 0, 't2t_device_info': {'num_async_replicas': 1}, '_evaluation_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_train_distribute': None, '_is_chief': True, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_environment': 'local', '_master': '', '_model_dir': 'gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model_hparam/28', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fc14bca0250>, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7fc14ad0e0c8>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:decode_hp.batch_size not specified; default=32\n",
      "INFO:tensorflow:Performing decoding from file (data/poetry/rumi_leads.txt).\n",
      "INFO:tensorflow:Getting sorted inputs\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/envs/py2env/bin/t2t-decoder\", line 17, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\n",
      "    _sys.exit(main(argv))\n",
      "  File \"/usr/local/envs/py2env/bin/t2t-decoder\", line 12, in main\n",
      "    t2t_decoder.main(argv)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/bin/t2t_decoder.py\", line 193, in main\n",
      "    decode(estimator, hp, decode_hp)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/bin/t2t_decoder.py\", line 93, in decode\n",
      "    checkpoint_path=FLAGS.checkpoint_path)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/decoding.py\", line 384, in decode_from_file\n",
      "    for elapsed_time, result in timer(result_iter):\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/utils/decoding.py\", line 378, in timer\n",
      "    item = next(gen)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 488, in predict\n",
      "    self._model_dir))\n",
      "ValueError: Could not find trained model in model_dir: gs://qwiklabs-gcp-c56dd36db3fa27d6/poetry/model_hparam/28.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# same as the above training job ...\n",
    "BEST_TRIAL=28  # CHANGE as needed.\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model_hparam/$BEST_TRIAL\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE \\\n",
    "  --hparams=\"num_hidden_layers=4,hidden_size=512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and all things that charming lay\n",
      "and yet anon repairs of oars\n",
      "the fishing organ shook the skies\n",
      "the wand'd agonies\n",
      "to school along the public public public public way\n",
      "and gentlemen of spain bewtie\n",
      "and many a hey nonny nonny nonny nonny\n",
      "and on the road\n",
      "if she will not seemely\n",
      "to show mean mean mean\n",
      "and ready harbinger\n",
      "ofter times in disport\n"
     ]
    }
   ],
   "source": [
    "%bash  \n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first three line. I'm showing the first line of the couplet provided to the model, how the AI model that we trained complets it and how Rumi completes it:\n",
    "<p>\n",
    "INPUT: where did the handsome beloved go <br/>\n",
    "AI: where art thou worse to me than dead <br/>\n",
    "RUMI: I wonder, where did that tall, shapely cypress tree go?\n",
    "<p>\n",
    "INPUT: he spread his light among us like a candle <br/>\n",
    "AI: like the hurricane eclipse <br/>\n",
    "RUMI: Where did he go? So strange, where did he go without me? <br/>\n",
    "<p>\n",
    "INPUT: all day long my heart trembles like a leaf <br/>\n",
    "AI: and through their hollow aisles it plays <br/>\n",
    "RUMI: All alone at midnight, where did that beloved go? \n",
    "<p>\n",
    "Oh wow. The couplets as completed are quite decent considering that:\n",
    "* We trained the model on American poetry, so feeding it Rumi is a bit out of left field.\n",
    "* Rumi, of course, has a context and thread running through his lines while the AI (since it was fed only that one line) doesn't. \n",
    "\n",
    "<p>\n",
    "\"Spreading light like a hurricane eclipse\" is a metaphor I won't soon forget. And it was created by a machine learning model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving poetry\n",
    "\n",
    "How would you serve these predictions? There are two ways:\n",
    "<ol>\n",
    "<li> Use [Cloud ML Engine](https://cloud.google.com/ml-engine/docs/deploying-models) -- this is serverless and you don't have to manage any infrastructure.\n",
    "<li> Use [Kubeflow](https://github.com/kubeflow/kubeflow/blob/master/user_guide.md) on Google Kubernetes Engine -- this uses clusters but will also work on-prem on your own Kubernetes cluster.\n",
    "</ol>\n",
    "<p>\n",
    "In either case, you need to export the model first and have TensorFlow serving serve the model. The model, however, expects to see *encoded* (i.e. preprocessed) data. So, we'll do that in the Python Flask application (in AppEngine Flex) that serves the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/envs/py2env/bin/t2t-exporter\", line 7, in <module>\n",
      "    from tensor2tensor.serving import export\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensor2tensor/serving/export.py\", line 29, in <module>\n",
      "    import tensorflow_hub as hub\n",
      "ImportError: No module named tensorflow_hub\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model_full2\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-exporter \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --t2t_usr_dir=./poetry/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "usage: saved_model_cli show [-h] --dir DIR [--all] [--tag_set TAG_SET]\n",
      "                            [--signature_def SIGNATURE_DEF_KEY]\n",
      "saved_model_cli show: error: argument --dir: expected one argument\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model_full2/export/Servo | tail -1)\n",
    "echo $MODEL_LOCATION\n",
    "saved_model_cli show --dir $MODEL_LOCATION --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mlengine.json\n"
     ]
    }
   ],
   "source": [
    "%writefile mlengine.json\n",
    "description: Poetry service on ML Engine\n",
    "autoScaling:\n",
    "    minNodes: 1  # We don't want this model to autoscale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting and deploying poetry v1 from  ... this will take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n",
      "This will delete version [v1]...\n",
      "\n",
      "Do you want to continue (Y/n)?  Please enter 'y' or 'n':  Please enter 'y' or 'n':  Please enter 'y' or 'n':  Please enter 'y' or 'n':  \n",
      "ERROR: (gcloud.ml-engine.versions.delete) NOT_FOUND: Field: name Error: The model resource: \"poetry\" was not found. Please create the Cloud ML model resource first by using 'gcloud ml-engine models create poetry'.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: \"The model resource: \\\"poetry\\\" was not found. Please create the\\\n",
      "      \\ Cloud ML model resource first by using 'gcloud ml-engine models create poetry'.\"\n",
      "    field: name\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"poetry\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model_full2/export/Servo | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "#gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud alpha ml-engine versions create --machine-type=mls1-highcpu-4 ${MODEL_VERSION} \\\n",
    "       --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=1.5 --config=mlengine.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Your current Cloud SDK version is: 212.0.0\n",
      "You will be upgraded to version: 224.0.0\n",
      "\n",
      "+----------------------------------------------------------------------------+\n",
      "|                     These components will be updated.                      |\n",
      "+-----------------------------------------------------+------------+---------+\n",
      "|                         Name                        |  Version   |   Size  |\n",
      "+-----------------------------------------------------+------------+---------+\n",
      "| BigQuery Command Line Tool                          |     2.0.36 | < 1 MiB |\n",
      "| BigQuery Command Line Tool (Platform Specific)      |     2.0.34 | < 1 MiB |\n",
      "| Cloud SDK Core Libraries                            | 2018.11.02 | 8.8 MiB |\n",
      "| Cloud SDK Core Libraries (Platform Specific)        | 2018.09.24 | < 1 MiB |\n",
      "| Cloud Storage Command Line Tool                     |       4.34 | 3.5 MiB |\n",
      "| Cloud Storage Command Line Tool (Platform Specific) |       4.34 | < 1 MiB |\n",
      "| gcloud Alpha Commands                               | 2018.09.04 | < 1 MiB |\n",
      "| gcloud cli dependencies                             | 2018.10.26 | 2.4 MiB |\n",
      "+-----------------------------------------------------+------------+---------+\n",
      "\n",
      "The following release notes are new in this upgrade.\n",
      "Please read carefully for information about new features, breaking changes,\n",
      "and bugs fixed.  The latest full release notes can be viewed at:\n",
      "  https://cloud.google.com/sdk/release_notes\n",
      "\n",
      "224.0.0 (2018-11-06)\n",
      "  Breaking Changes\n",
      "      o **(Container Builder)** Removed deprecated container-builder-local\n",
      "        component. Use cloud-build-local instead.\n",
      "      o **(Container Builder)** Removed gcloud container builds command\n",
      "        group. Callers will be redirected to use gcloud builds instead.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.79. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "      o Updated the Go SDK to version 1.9.70. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/go/release-notes\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --hostname flag of gcloud compute instances create to beta.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "223.0.0 (2018-10-30)\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.78. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "\n",
      "      o Updated the Java SDK to version 1.9.68. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/java/release-notes\n",
      "\n",
      "      o Updated the Go SDK to version 1.9.69. Fix a bug in windows support\n",
      "        for go.mod deployments for the Go 1.11 runtime\n",
      "\n",
      "  Cloud Access Context Manager\n",
      "      o Promoted the gcloud access-context-manager command group to BETA\n",
      "\n",
      "  Cloud Build\n",
      "      o Added default behavior for gcloud builds submit to assume\n",
      "        --config=cloudbuild.yaml unless either --tag or --config is specified.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted Node Auto-Provisioning to beta. Use\n",
      "        --enable-autoprovisioning with\n",
      "    gcloud beta container clusters create or gcloud beta container clusters\n",
      "    update to enable this feature.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "222.0.0 (2018-10-23)\n",
      "  App Engine\n",
      "      o Fixed a bug that caused gcloud app logs tail to crash when the logs\n",
      "        were too large.\n",
      "      o Updated the Java SDK to version 1.9.67. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/java/release-notes\n",
      "\n",
      "  Cloud Build\n",
      "      o Updated cloud-build-local to 0.4.2. For details, see\n",
      "        <https://github.com/GoogleCloudPlatform/cloud-build-local/releases>.\n",
      "\n",
      "  Cloud Composer\n",
      "      o Added two mutually exclusive flags to gcloud beta composer\n",
      "        environments create:\n",
      "        * --airflow-version - specifies the Apache Airflow version for the\n",
      "          created environment\n",
      "        * --image-version - specifies the image version for the created\n",
      "          environment\n",
      "\n",
      "  Cloud DNS\n",
      "      o Added support for private zones to gcloud beta dns managed-zones. Use\n",
      "        the new --visibility and --networks flags to configure zone visibility.\n",
      "\n",
      "  Cloud Dataflow\n",
      "      o Updated error message for cancel and drain commands to include\n",
      "        possibility of\n",
      "    --region flag inconsistency.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Updated gcloud <alpha|beta> sql connect to connect to V2 instances\n",
      "        through the Cloud SQL Proxy Component.\n",
      "      o Added the --port flag to gcloud <alpha|beta> sql connect to allow for\n",
      "        connecting to the Cloud SQL Proxy through a custom port.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --container-mount-disk- flags of gcloud compute instances\n",
      "        <create-with-container|update-container> to beta. These flags enable\n",
      "        mounting disks to containers running on VMs.\n",
      "\n",
      "  Container Builder\n",
      "      o Reminder: use gcloud builds instead of gcloud container builds; the\n",
      "        deprecated command group gcloud container builds will be removed on or\n",
      "        after 2018-10-31.\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Promoted the gcloud firebase test ios command group to GA.\n",
      "\n",
      "  Identity and Access Management\n",
      "      o Modified gcloud iam service-accounts keys create such that in the\n",
      "        case a user does not have write access to the specified output file,\n",
      "        the command will not be executed.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "221.0.0 (2018-10-16)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Removed the deprecated host positional argument from\n",
      "        gcloud sql users <create|delete|set-password>.\n",
      "\n",
      "  App Engine\n",
      "      o Fixed a bug where environment variables with values of 'on' or 'off'\n",
      "        were ending up as 'true' or 'false' when deployed.\n",
      "      o gcloud app logs <read|tail> now displays stdout and stderr from the\n",
      "        App Engine standard environment Python 3.7, PHP 7.2, Go 1.11, Java 8,\n",
      "        and Node.js 8 runtimes by default, or explicitly by supplying\n",
      "        --logs=<stdout|stderr>.\n",
      "\n",
      "  BigQuery\n",
      "      o Added flags for setting a default table partition expiration for a\n",
      "        dataset.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Promoted the --no-address flag of gcloud dataproc clusters create and\n",
      "    gcloud dataproc workflow-templates set-managed-cluster to GA.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Updated the cloud_sql_proxy component to version 1.13.0. Please visit\n",
      "        the following release notes for details:\n",
      "        https://github.com/GoogleCloudPlatform/cloudsql-proxy/releases\n",
      "\n",
      "  Cloud Spanner\n",
      "      o Added --enable-partitioned-dml flag for gcloud beta spanner\n",
      "        execute-sql\n",
      "\n",
      "  Compute Engine\n",
      "      o Added gcloud beta compute instances get-guest-attributes command for\n",
      "        retrieving guest attributes.\n",
      "      o Promoted --response flag of gcloud compute health-checks for HTTP/S\n",
      "        commands to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "220.0.0 (2018-10-09)\n",
      "  Breaking Changes\n",
      "      o **(Kubernetes Engine)** Modified output of gcloud container clusters\n",
      "        list for DEGRADED clusters to include reason for degradation.\n",
      "      o **(Kubernetes Engine)** Starting in 1.12, new node pools (and default\n",
      "        node pools in new clusters) will be created with their legacy Compute\n",
      "        Engine instance metadata APIs disabled by default.\n",
      "        * To create a new node pool (or default pool in a new cluster) with\n",
      "          legacy metadata APIs disabled, use the flag --metadata\n",
      "          disable-legacy-endpoints=true. See:\n",
      "          <https://cloud.google.com/kubernetes-engine/docs/how-to/protecting-cluster-metadata#disable-legacy-apis>\n",
      "\n",
      "  Cloud SDK\n",
      "      o Updated the storage/chunk_size property. Commands that upload to\n",
      "        Google Cloud Storage can now control the upload/download chunksize\n",
      "        using this property.\n",
      "      o Some commands no longer call gsutil in their implementation in order\n",
      "        to support Python 3. The gsutil implementation is now deprecated. Use\n",
      "        the storage/use_gsutil property to temporarily get this behavior back.\n",
      "        This property and its old implementation will eventually be removed.\n",
      "        The following commands are affected by this change:\n",
      "        * functions deploy\n",
      "        * compute images import\n",
      "        * dataproc jobs submit <pyspark|hadoop|pig|hive|spark|spark-sql>\n",
      "        * composer environments storage <dags|data|plugins>\n",
      "          <delete|export|import>\n",
      "      o Added functionality to gcloud beta help that allows running a search\n",
      "        for terms of interest within the help text of gcloud commands. For more\n",
      "        information, run $ gcloud beta help --help.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Java SDK to version 1.9.66. Please visit the following\n",
      "        release notes for details:\n",
      "        <https://cloud.google.com/appengine/docs/java/release-notes>\n",
      "      o Updated the Python SDK to version 1.9.77. Please visit the following\n",
      "        release notes for details:\n",
      "        <https://cloud.google.com/appengine/docs/python/release-notes>\n",
      "\n",
      "  Cloud Build\n",
      "      o Added a warning message to gcloud builds submit for builds submitted\n",
      "        with\n",
      "    configs that don't specify a logging option. See\n",
      "    <https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds#loggingmode>.\n",
      "\n",
      "  Cloud Composer\n",
      "      o Added the --python-version flag to gcloud beta composer environments\n",
      "        create to specify the Python version used within the created\n",
      "        environment.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added the --gce-pd-kms-key flag to dataproc clusters create to enable\n",
      "        protecting clusters with Google Cloud KMS encryption.\n",
      "\n",
      "  Cloud Key Management Service\n",
      "      o Modified gcloud kms locations list to display information about the\n",
      "        availability of Hardware Security Modules in each location.\n",
      "\n",
      "  Cloud Spanner\n",
      "      o Updated gcloud spanner execute-sql to accept DML statements.\n",
      "\n",
      "  Compute Engine\n",
      "      o Added support for managed ssl certificates to gcloud beta compute\n",
      "        ssl-certificates.\n",
      "\n",
      "  Internet of Things\n",
      "      o Added --log-level flag for gcloud iot <devices|registries>\n",
      "        <create|update> for alpha and beta.\n",
      "      o Added gcloud iot commands to alpha and beta.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "219.0.1 (2018-10-02)\n",
      "  Cloud SDK\n",
      "      o Updated to a new version of ruamel that fixes Unicode issues\n",
      "        (https://issuetracker.google.com/issues/113348923) on OS X and Windows.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.76. Please visit the following\n",
      "        release notes for details:\n",
      "        <https://cloud.google.com/appengine/docs/python/release-notes>\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted compute routers nats to beta.\n",
      "      o Promoted 'gcloud compute interconnects get-diagnostics' to beta.\n",
      "      o Promoted the following flags to GA to support using KMS keys to\n",
      "        protect disks and images:\n",
      "        * --kms-key- flags of gcloud compute <disks|images>\n",
      "        * --boot-disk-kms- flags of gcloud compute\n",
      "          <instances|instance-templates>.\n",
      "      o Promoted gcloud compute instance-templates create-with-container from\n",
      "        beta to GA.\n",
      "\n",
      "  Interactive\n",
      "      o Promoted gcloud interactive to beta.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added --enable-private-nodes, --enable-private-endpoint, and\n",
      "        --master-ipv4-cidr flags to gcloud container clusters create.\n",
      "      o Added --internal-ip flag to gcloud container clusters\n",
      "        get-credentials.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "218.0.0 (2018-09-25)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Added the PRIVATE_NETWORK column to the gcloud sql\n",
      "        instances list default format and renamed ADDRESS to PRIMARY_ADDRESS.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added gcloud beta dataproc clusters export to enable exporting a\n",
      "        cluster's configuration to a YAML file.\n",
      "      o Added gcloud beta dataproc clusters import to enable creating a\n",
      "        cluster from configuration in a YAML file.\n",
      "      o Added --optional-components flag to gcloud beta dataproc clusters\n",
      "        create command.\n",
      "      o Promoted gcloud dataproc\n",
      "        <clusters|jobs|operations|workflow-templates> <get|set>-iam-policy to\n",
      "        GA.\n",
      "      o Promoted gcloud dataproc workflow-templates to GA.\n",
      "\n",
      "  Cloud Scheduler\n",
      "      o Promoted gcloud scheduler to beta.\n",
      "\n",
      "  Cloud Tools For PowerShell\n",
      "      o Updated Cloud Tools for PowerShell to version 1.0.1.10.\n",
      "        * Fixed a bug where fixed key metadata did not work with Google Cloud\n",
      "          Storage cmdlets.\n",
      "\n",
      "  Compute Engine\n",
      "      o Added the --storage-location flag to gcloud beta compute disks\n",
      "        snapshot\n",
      "    to specify location region of a snapshot.\n",
      "\n",
      "  Container Analysis\n",
      "      o Promoted the following alpha flags in gcloud container images\n",
      "        describe to beta.\n",
      "        * --metadata-filter\n",
      "        * --show-build-details\n",
      "        * --show-package-vulnerability\n",
      "        * --show-image-basis\n",
      "        * --show-deployment\n",
      "        * --show-all-metadata\n",
      "      o Promoted the following alpha flags in gcloud container images\n",
      "        list-tags to beta.\n",
      "        * --occurrence-filter\n",
      "        * --show-occurrences\n",
      "        * --show-occurrences-from For more information about occurrences, see\n",
      "          <https://cloud.google.com/container-registry/docs/container-analysis>.\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Fixed bug where --environment-variables did not work in the 217.0.0\n",
      "        release.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added --metadata and --metadata-from-file flags to gcloud\n",
      "        <node-pools|clusters> create.\n",
      "      o Added --internal-ip flag to gcloud beta container clusters\n",
      "        get-credentials.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "217.0.0 (2018-09-18)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Services)** Renamed --reserved-ranges to ranges in gcloud\n",
      "        beta services vpc-peerings.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Java SDK to version 1.9.65. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/java/release-notes\n",
      "\n",
      "  Cloud SQL\n",
      "      o Promoted the --network flag of gcloud sql instances <create|patch> to\n",
      "        beta.\n",
      "\n",
      "  Cloud Storage\n",
      "      o Updated gsutil component to 4.34\n",
      "\n",
      "  Compute Engine\n",
      "      o Added --prefix-length, --purpose and --network flags to BETA for\n",
      "        gcloud compute addresses create to support reserving IP range from\n",
      "        virtual network for peering.\n",
      "      o Added the description property to the --create-disk flag of gcloud\n",
      "        compute <instances|instance-templates> create.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added --metadata and --metadata-from-file flags to gcloud beta\n",
      "        <node-pools|clusters> create.\n",
      "      o Updated Google Kubernetes Engine's kubectl from version 1.9.7 to\n",
      "        1.10.7.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "216.0.0 (2018-09-11)\n",
      "  App Engine\n",
      "      o Updated the Go SDK to version 1.9.68. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/go/release-notes\n",
      "\n",
      "  Cloud Bigtable\n",
      "      o Promoted the following commands to GA:\n",
      "        * gcloud bigtable instances command group\n",
      "        * gcloud bigtable clusters list\n",
      "        * gcloud bigtable clusters describe\n",
      "\n",
      "  Cloud Memorystore\n",
      "      o Promoted gcloud redis to GA.\n",
      "\n",
      "  Cloud Services\n",
      "      o Deprecated gcloud services operations list in beta and GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --internal-ip flag of gcloud compute scp to beta.\n",
      "      o Promoted --disabled flag of gcloud compute firewall-rules to GA.\n",
      "      o Fixed bug preventing gcloud compute <ssh|scp> from finding an\n",
      "        instance's external IP address when configured with multiple network\n",
      "        interfaces.\n",
      "      o Promoted gcloud compute instances\n",
      "        <create-with-container|update-container> to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "215.0.0 (2018-09-05)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Removed the default value of the --database-version\n",
      "        flag of gcloud sql instances create, allowing the API to select the\n",
      "        value.\n",
      "\n",
      "  App Engine\n",
      "      o Added python 3 support for gcloud app and gcloud domains.\n",
      "      o Added the --no-cache flag to gcloud beta app deploy for Second\n",
      "        Generation runtimes, to disable the build cache during deployment.\n",
      "      o Updated the Python SDK to version 1.9.75. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "\n",
      "  Cloud Datalab\n",
      "      o Updated the datalab component to the 20180823 release. Released\n",
      "        changes are documented in its tracking issue at\n",
      "        https://github.com/googledatalab/datalab/issues/2068\n",
      "        (https://github.com/googledatalab/datalab/issues/2068).\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Fixed an issue preventing the creation of clusters with SSD in GA.\n",
      "\n",
      "  Cloud Firestore\n",
      "      o Added gcloud beta firestore and gcloud beta firestore operations for\n",
      "        managing cloud firestore imports and exports.\n",
      "\n",
      "  Cloud Functions\n",
      "      o Added --service-account flag to gcloud alpha functions deploy.\n",
      "\n",
      "  Cloud Pub/Sub\n",
      "      o Promoted Snapshot & Seek features to beta. These features allow users\n",
      "        to create snapshots of subscription backlog state, and later restore\n",
      "        that state.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Fixed a bug preventing --backup and --enable-bin-log flags from being\n",
      "        sent together when using the gcloud sql instances create command.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --internal-ip flag of gcloud compute scp to beta.\n",
      "      o Promoted --disabled flag of gcloud compute firewall-rules to GA.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promote gcloud alpha container subnets list-usable to Beta.\n",
      "      o Add secondaryIpRanges to the output of gcloud beta container subnets\n",
      "        list-usable.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "214.0.0 (2018-08-28)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Bigtable)** Modified the arguments accepted by cbt\n",
      "        createappprofile and cbt updateappprofile in the following ways:\n",
      "        * Removed etag argument from createappprofile.\n",
      "        * Renamed allow-transactional-writes option as transactional-writes.\n",
      "        * Added a force option to ignore warnings.\n",
      "      o **(Cloud Bigtable)** Modified the specification for routing policies.\n",
      "        A routing policy can be either \"route-any\" (previously of\n",
      "        \"multi_cluster_routing_use_any\") or \"route-to=<cluster-id>\".\n",
      "      o **(Compute Engine)** Deprecated gcloud compute interconnects\n",
      "        attachments create. Please use gcloud compute interconnects attachments\n",
      "        dedicated create instead.\n",
      "      o **(Compute Engine)** Removed deprecated --mode flag from gcloud\n",
      "        compute networks create. Use --subnet-mode instead.\n",
      "      o **(Compute Engine)** Removed deprecated gcloud compute networks\n",
      "        switch-mode command. Use gcloud compute networks update\n",
      "        --switch-to-custom-mode instead.\n",
      "      o **(Compute Engine)** Removed deprecated gcloud compute xpn command\n",
      "        group. Use gcloud compute shared-vpc instead.\n",
      "\n",
      "  Cloud Bigtable\n",
      "      o Restored the output of the cbt count command that was inadvertently\n",
      "        removed in the previous release.\n",
      "\n",
      "  Cloud Datalab\n",
      "      o Updated the datalab component to the 20180820 release. Released\n",
      "        changes are documented in its tracking issue at\n",
      "        https://github.com/googledatalab/datalab/issues/2064\n",
      "        (https://github.com/googledatalab/datalab/issues/2064).\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added SCHEDULED_DELETE column to gcloud beta dataproc clusters list\n",
      "        command output.\n",
      "\n",
      "  Cloud Datastore Emulator\n",
      "      o Released Cloud Datastore Emulator version 2.0.2.\n",
      "        * Improved backward compatibility with App Engine local development\n",
      "          by keeping auto generated indexes in index file generated from\n",
      "          previous runs.\n",
      "\n",
      "  Cloud Functions\n",
      "      o Promoted --runtime flag of gcloud functions deploy to GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted the following flags to GA:\n",
      "        * --network-tier of gcloud compute <addresses|forwarding-rules>\n",
      "          create\n",
      "        * --default-network-tier of gcloud compute project-info update\n",
      "        * --network-tier of gcloud compute instances\n",
      "          <add-access-config|create>\n",
      "        * --network-tier of gcloud compute instance-templates create\n",
      "      o Promoted gcloud compute instances simulate-maintenance-event to GA.\n",
      "      o Promoted <get|set\\>-iam-policy and <add|remove\\>-iam-policy-bindings\n",
      "        to beta in the following commands groups:\n",
      "        * gcloud compute sole-tenancy node-groups\n",
      "        * gcloud compute sole-tenancy node-templates\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted --disk-type flag of gcloud container <clusters|node-pools>\n",
      "        create to GA.\n",
      "      o Promoted --default-max-pods-per-node flag of gcloud container\n",
      "        clusters create to beta.\n",
      "      o Promoted --max-pods-per-node flag of gcloud container node-pools\n",
      "        create to beta.\n",
      "      o Modified --monitoring-service flag of gcloud containers clusters\n",
      "        update to enable Google Cloud Monitoring service with Kubernetes-native\n",
      "        resource model.\n",
      "      o Modified --logging-service flag of gcloud containers clusters update\n",
      "        to enable Google Cloud Logging service with Kubernetes-native resource\n",
      "        model.\n",
      "      o Modified output of gcloud beta container clusters list for DEGRADED\n",
      "        clusters to include reason for degradation.\n",
      "      o Added --enable-private-nodes and --enable-private-endpoint to gcloud\n",
      "        beta container clusters create.\n",
      "      o Deprecated --private-cluster flag of gcloud beta container clusters\n",
      "        create; use --enable-private-nodes instead.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "213.0.0 (2018-08-21)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Datastore)** Deprecated gcloud datastore\n",
      "        <create|cleanup>-indexes. Use gcloud datastore indexes <create|cleanup>\n",
      "        instead.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.74. Please visit the following\n",
      "        release notes for details:\n",
      "        <https://cloud.google.com/appengine/docs/python/release-notes>\n",
      "\n",
      "  Cloud Datastore\n",
      "      o Promoted gcloud datastore indexes command group to GA.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Added the --timeout flag to gcloud sql operations wait, to support\n",
      "        specifying a custom timeout or allow the command to wait indefinitely.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted --disk-type flag of gcloud container <clusters|node-pools>\n",
      "        create to GA.\n",
      "      o Modified --monitoring-service flag of gcloud containers clusters\n",
      "        update to enable Google Cloud Monitoring service with Kubernetes-native\n",
      "        resource model.\n",
      "      o Modified --logging-service flag of gcloud containers clusters update\n",
      "        to enable Google Cloud Logging service with Kubernetes-native resource\n",
      "        model.\n",
      "      o Modified output of gcloud beta container clusters list for DEGRADED\n",
      "        clusters to include reason for degradation.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "#============================================================#\n",
      "#= Creating update staging area                             =#\n",
      "#============================================================#\n",
      "#= Uninstalling: BigQuery Command Line Tool                 =#\n",
      "#============================================================#\n",
      "#= Uninstalling: BigQuery Command Line Tool (Platform Sp... =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud SDK Core Libraries                   =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud SDK Core Libraries (Platform Spec... =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud Storage Command Line Tool            =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud Storage Command Line Tool (Platfo... =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud Alpha Commands                      =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud cli dependencies                    =#\n",
      "#============================================================#\n",
      "#= Installing: BigQuery Command Line Tool                   =#\n",
      "#============================================================#\n",
      "#= Installing: BigQuery Command Line Tool (Platform Spec... =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud SDK Core Libraries                     =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud SDK Core Libraries (Platform Specific) =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud Storage Command Line Tool              =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud Storage Command Line Tool (Platform... =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud Alpha Commands                        =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud cli dependencies                      =#\n",
      "#============================================================#\n",
      "#= Creating backup and activating new installation          =#\n",
      "#============================================================#\n",
      "\n",
      "Performing post processing steps...\n",
      "...........................................done.\n",
      "\n",
      "==> Start a new shell for the changes to take effect.\n",
      "\n",
      "\n",
      "Update done!\n",
      "\n",
      "To revert your SDK to the previously installed version, you may run:\n",
      "  $ gcloud components update --version 212.0.0\n",
      "\n",
      "\n",
      "All components are up to date.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud components update --quiet\n",
    "gcloud components install alpha --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n",
      "ERROR: (gcloud.alpha.ml-engine.versions.create) argument --origin: expected one argument\n",
      "Usage: gcloud alpha ml-engine versions create VERSION --model=MODEL [optional flags]\n",
      "  optional flags may be  --accelerator | --async | --config | --description |\n",
      "                         --framework | --help | --labels | --machine-type |\n",
      "                         --model-class | --origin | --package-uris |\n",
      "                         --python-version | --runtime-version | --staging-bucket\n",
      "\n",
      "For detailed information on this command and its flags, run:\n",
      "  gcloud alpha ml-engine versions create --help\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"poetry\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model_full2/export/Servo | tail -1)\n",
    "gcloud alpha ml-engine versions create --machine-type=mls1-highcpu-4 ${MODEL_VERSION} \\\n",
    "       --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=1.5 --config=mlengine.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kubeflow\n",
    "\n",
    "Follow these instructions:\n",
    "* On the GCP console, launch a Google Kubernetes Engine (GKE) cluster named 'poetry' with 2 nodes, each of which is a n1-standard-2 (2 vCPUs, 7.5 GB memory) VM\n",
    "* On the GCP console, click on the Connect button for your cluster, and choose the CloudShell option\n",
    "* In CloudShell, run: \n",
    "    ```\n",
    "    git clone https://github.com/GoogleCloudPlatform/training-data-analyst`\n",
    "    cd training-data-analyst/courses/machine_learning/deepdive/09_sequence\n",
    "    ```\n",
    "* Look at [`./setup_kubeflow.sh`](setup_kubeflow.sh) and modify as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AppEngine\n",
    "\n",
    "What's deployed in Cloud ML Engine or Kubeflow is only the TensorFlow model. We still need a preprocessing service. That is done using AppEngine.  Edit application/app.yaml appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: python\r\n",
      "env: flex\r\n",
      "entrypoint: gunicorn -b :$PORT main:app\r\n",
      "service: mlpoetry\r\n",
      "\r\n",
      "handlers:\r\n",
      "- url: /\r\n",
      "  script: main.app\r\n",
      "- url: /.*\r\n",
      "  script: main.app\r\n",
      "\r\n",
      "env_variables:\r\n",
      "  MODEL_NAME: poetry\r\n",
      "  PROJECT_ID: cloud-training-demos\r\n",
      "  VERSION_NAME: v1\r\n",
      "  PROBLEM_NAME: poetry_line_problem\r\n",
      "  T2T_USR_DIR: instance/poetry/trainer\r\n",
      "  HPARAMS: transformer_poetry\r\n",
      "  DATADIR: gs://cloud-training-demos-ml/poetry/data\r\n"
     ]
    }
   ],
   "source": [
    "!cat application/app.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "cd application\n",
    "#gcloud app create  # if this is your first app\n",
    "#gcloud app deploy --quiet --stop-previous-version app.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visit https://mlpoetry-dot-cloud-training-demos.appspot.com and try out the prediction app!\n",
    "\n",
    "<img src=\"diagrams/poetry_app.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2018 Google Inc. Licensed under the Apache License, Version 2.0 (the \\\"License\\\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
